{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "- All the document in the collection share the same set of topics, but each document exhibits those topics in different proportions\n",
    "\n",
    "- Each word in each document is drawn from one of the topics, where the selected topic is chosen from the per-document distribution over topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the library, `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', timeout('timed out',))': /simple/gensim/\u001b[0m\n",
      "  Downloading gensim-2.1.0.tar.gz (15.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 15.1MB 71kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.3 in /home/rs/jpatro/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.7.0 in /home/rs/jpatro/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/rs/jpatro/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Collecting smart_open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.3.tar.gz\n",
      "Requirement already satisfied: boto>=2.32 in /home/rs/jpatro/anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart_open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /home/rs/jpatro/anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: gensim, smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for gensim ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/rs/jpatro/.cache/pip/wheels/6f/f0/82/34d132e5a03ca765ec7f006d5f38ec7d749274a3f59c3fa5d0\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/rs/jpatro/.cache/pip/wheels/b0/81/ad/856aade935fceaab491a800ec4de58edb8642afa4c4ba91a00\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/rs/jpatro/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built gensim smart-open bz2file\n",
      "Installing collected packages: bz2file, smart-open, gensim\n",
      "Successfully installed bz2file-0.98 gensim-2.1.0 smart-open-1.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tiny corpus of nine documents, each consisting of only a single sentence.\n",
    "\n",
    "First, let’s tokenize the documents, remove common words (using a toy stoplist) as well as words that only appear once in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Find all words which occur more than 1 time in the corpus (of 9 documents)\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['human', 'interface', 'computer', 'survey', 'user']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('deerwester.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human': 0, 'interface': 1, 'computer': 2, 'survey': 3, 'user': 4, 'system': 5, 'response': 6, 'time': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `doc2bow()` simply counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. The sparse vector `[(word_id, 1), (word_id, 1)]` therefore reads: in the document *“Human computer interaction”*, the words *\"computer\"* and *\"human\"*, identified by an integer id given by the built dictionary, appear once; the other ten dictionary words appear (implicitly) zero times. Check their id at the dictionary displayed in the previous cell and see that they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(1, 1), (4, 1), (5, 1), (8, 1)]\n",
      "[(0, 1), (5, 2), (8, 1)]\n",
      "[(4, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(3, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('deerwester.mm', corpus)  # store to disk, for later use\n",
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "import numpy\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (7, 1)],\n",
       " [(0, 2), (1, 1), (3, 1), (8, 1)],\n",
       " [(1, 1), (3, 1), (6, 1), (9, 1)],\n",
       " [(0, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (10, 1), (13, 1)],\n",
       " [(0, 1), (12, 1)],\n",
       " [(0, 1), (10, 1), (12, 1), (14, 1)],\n",
       " [(13, 1), (14, 1)],\n",
       " [(0, 1), (14, 1), (15, 1)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [['bank','river','shore','water'],\n",
    "        ['river','water','flow','fast','tree'],\n",
    "        ['bank','water','fall','flow'],\n",
    "        ['bank','bank','water','rain','river'],\n",
    "        ['river','water','mud','tree'],\n",
    "        ['money','transaction','bank','finance'],\n",
    "        ['bank','borrow','money'], \n",
    "        ['bank','finance'],\n",
    "        ['finance','money','sell','bank'],\n",
    "        ['borrow','sell'],\n",
    "        ['bank','loan','sell']]\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(1) # setting random seed to get the same results each time.\n",
    "model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.206*\"bank\" + 0.111*\"water\" + 0.083*\"river\" + 0.083*\"sell\" + 0.065*\"flow\" + 0.065*\"borrow\" + 0.064*\"money\" + 0.051*\"finance\" + 0.049*\"tree\" + 0.041*\"rain\"'),\n",
       " (1,\n",
       "  '0.136*\"bank\" + 0.103*\"finance\" + 0.101*\"water\" + 0.099*\"river\" + 0.084*\"money\" + 0.061*\"transaction\" + 0.061*\"tree\" + 0.055*\"mud\" + 0.054*\"sell\" + 0.043*\"shore\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.097919175370194289), (1, 0.081618646341786241)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_term_topics('water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.037650404355932685), (1, 0.083387133953968046)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_term_topics('finance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (3, 1)]\n",
      "[(0, [0, 1]), (3, [0, 1])]\n",
      "[(0, 0.82570570581386704), (1, 0.17429429418613296)]\n"
     ]
    }
   ],
   "source": [
    "bow_water = ['bank','water','bank']\n",
    "bow_finance = ['bank','finance','bank']\n",
    "\n",
    "bow = model.id2word.doc2bow(bow_water) # convert to bag of words format first\n",
    "print (bow)\n",
    "doc_topics, word_topics, phi_values = model.get_document_topics(bow, per_word_topics=True)\n",
    "\n",
    "print (word_topics)\n",
    "print (doc_topics)\n",
    "#print (phi_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what does that output mean? It means that like `word_type 1`, our `word_type` `3`, which is the word `bank`, is more likely to be in `topic_0` than `topic_1`.\n",
    "\n",
    "You must have noticed that while we unpacked into `doc_topics` and `word_topics`, there is another variable - `phi_values`. Like the name suggests, phi_values contains the phi values for each topic for that particular word, scaled by feature length. Phi is essentially the probability of that word in that document belonging to a particular topic. The next few lines should illustrate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(0, 0.92486455564294345), (1, 0.075135444357056574)]),\n",
       " (3, [(0, 1.5817120973072454), (1, 0.41828790269275457)])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that `word_type` 0 has the following phi_values for each of the topics. \n",
    "What is intresting to note is `word_type` 3 - because it has 2 occurences (i.e, the word `bank` appears twice in the bow), we can see that the scaling by feature length is very evident. The sum of the phi_values is 2, and not 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
