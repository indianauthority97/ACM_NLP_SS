{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naive Baye's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding word counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to calculate the probabilities of each classification, and the probabilities of each feature falling into each classification.\n",
    "\n",
    "Here, all we have is one long string. The easiest way to generate features from text is to split the text up into words. Each word in a review will then be a feature that we can then work with. In order to do this, we’ll split the reviews based on whitespace.\n",
    "\n",
    "We’ll then count up how many times each word occurs in the negative reviews, and how many times each word occurs in the positive reviews. This will allow us to eventually compute the probabilities of a new review belonging to each class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rs/jpatro/NotebooksNLP-master\n",
      "Negative text sample: This movie is without a doubt the worst horror movie I've ever seen. And that's saying a lot, considering I've seen such stinkers like Demon of Paradise, Lovers' Lane, and Bloody Murder (which is a close second). However, I love bad horror movies, and as you can tell from my username, this one really sticks out. At times there's nothing more entertaining than a poorly made slasher flick. As for this film, the opening scene in which a woman gets fried in a tanning booth appears to have no bearing\n",
      "Positive text sample: I rented this movie this past weekend, cranked up the surround sound system, and got some great sound from special affects. This movie is a great movie rental, the special affects where enough to scare my fiance, but I noticed some looked suprisingly computer generated. I didn't go to the movies and see this, but its a scary late night don't feel like going out movie. I would recommend it! One reviewer notes that it does not seem to matter what Welles actually says or does, he moves you. I concu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "i=0\n",
    "print (os.getcwd())\n",
    "posreviews=[]\n",
    "positive_text = \" \"\n",
    "negative_text = \" \"\n",
    "i=0\n",
    "for fileName in os.listdir(\"mr/train/pos\"):\n",
    "    fo=open(\"mr/train/pos/%s\" % fileName,\"r\")\n",
    "    #print fo.name\n",
    "    #os.rename(\"mr/train/neg/%s\" % fileName,\"mr/train/neg/%d.txt\" %(i))\n",
    "    str= fo.read()\n",
    "    #print \"%d. %s\" % (i,str)\n",
    "    posreviews.append(str)\n",
    "    #print \"%d . %s\" %(i,positive_text)\n",
    "    #if i>1000: \n",
    "     #   break\n",
    "    fo.close()\n",
    "    i=i+1\n",
    "positive_text= \" \".join(posreviews)\n",
    "#print positive_text\n",
    "negreviews=[]\n",
    "i=0\n",
    "for fileName in os.listdir(\"mr/train/neg\"):\n",
    "    fo=open(\"mr/train/neg/%s\" % fileName,\"r\")\n",
    "    #os.rename(\"mr/train/neg/%s\" % fileName,\"mr/train/neg/%d.txt\" %(i))\n",
    "    negreviews.append(fo.read())\n",
    "    #if i>1000: \n",
    "     #   break\n",
    "    i=i+1\n",
    "negative_text=\" \".join(negreviews)\n",
    "#print negative_text\n",
    "\n",
    "def count_text(text):\n",
    "  # Split text into words based on whitespace.  Simple but effective.\n",
    "  words = re.split(\"\\s+\", text)\n",
    "  # Count up the occurence of each word.; \n",
    "  return Counter(words)\n",
    "\n",
    "# Generate word counts for negative tone.\n",
    "negative_counts = count_text(negative_text)\n",
    "#print (negative_counts)\n",
    "# Generate word counts for positive tone.\n",
    "positive_counts = count_text(positive_text)\n",
    "\n",
    "print(\"Negative text sample: {0}\".format(negative_text[:500]))\n",
    "print(\"Positive text sample: {0}\".format(positive_text[:500]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the word counts, we just have to convert them to probabilities and multiply them out to get the predicted classification. Let’s say we wanted to find the probability that the review didn't like it expresses a negative sentiment. We would find the total number of times the word didn't occured in the negative reviews, and divide it by the total number of words in the negative reviews to get the probability of x given y. We would then do the same for like and it. We would multiply all three probabilities, and then multiply by the probability of any document expressing a negative sentiment to get our final probability that the sentence expresses negative sentiment.\n",
    "\n",
    "We would do the same for positive sentiment, and then whichever probability is greater would be the class that the review is assigned to.\n",
    "\n",
    "To do all this, we’ll need to compute the probabilities of each class occuring in the data, and then make a function to compute the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good nice Well Done - Positive (pos score=8.0637844491572e-18, neg score=1.5364951176285029e-18)\n",
      "Movie was junk, useless, good for nothing, sheer waste of time and money. - Negative (pos score=6.353651426403834e-48, neg score=2.565937644891587e-45)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We need these counts to use for smoothing when computing the prediction.\n",
    "positive_review_count = len(posreviews)\n",
    "negative_review_count = len(negreviews)\n",
    "# These are the class probabilities (we saw them in the formula as P(y)).\n",
    "prob_positive = positive_review_count / float(len(posreviews) + len(negreviews))\n",
    "prob_negative = negative_review_count / float(len(posreviews) + len(negreviews))\n",
    "\n",
    "def make_class_prediction(text, counts, class_prob, class_count):\n",
    "  prediction = 1.0\n",
    "  text_counts = Counter(re.split(\"\\s+\", text))\n",
    "  #print (text_counts)\n",
    "  for word in text_counts:\n",
    "      # For every word in the text, we get the number of times that word occured in the reviews for a given class, add 1 to smooth the value, and divide by the total number of words in the class (plus the class_count to also smooth the denominator).\n",
    "      # Smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data.\n",
    "      # We also smooth the denominator counts to keep things even.\n",
    "      # print \"%d, %d,%d\" %(text_counts.get(word), counts.get(word,0), sum(counts.values()))\n",
    "      # print (word,'-', text_counts.get(word))\n",
    "      # print (word,'-', (counts.get(word,0)))\n",
    "      #print (word,'-', class_count)\n",
    "      prediction *=  text_counts.get(word) * ((counts.get(word,0) + 1) / float(sum(counts.values()) + class_count))\n",
    "      #print prediction\n",
    "  # Now we multiply by the probability of the class existing in the documents.\n",
    "  return prediction * class_prob\n",
    "\n",
    "# As you can see, we can now generate probabilities for which class a given review is part of.\n",
    "# The probabilities themselves aren't very useful -- we make our classification decision based on which value is greater.\n",
    "#print(\"Review: {0}\".format(reviews[0][0]))\n",
    "#text=\"Movie was junk, useless, good for nothing, sheer waste of time and money.\"\n",
    "text=\"Good nice Well Done\"\n",
    "#print(\"Negative prediction: {0}\".format(make_class_prediction(text, negative_counts, prob_negative, negative_review_count))\n",
    "neg=make_class_prediction(text, negative_counts, prob_negative, negative_review_count)\n",
    "pos=make_class_prediction(text, positive_counts, prob_positive, positive_review_count)\n",
    "if pos>neg:\n",
    "   print (\"%s - Positive (pos score={0}, neg score={1})\".format(pos,neg) %(text))\n",
    "else:\n",
    "   print (\"%s - Negative (pos score={0}, neg score={1})\".format(pos,neg) %(text))\n",
    "text=\"Movie was junk, useless, good for nothing, sheer waste of time and money.\"\n",
    "neg=make_class_prediction(text, negative_counts, prob_negative, negative_review_count)\n",
    "pos=make_class_prediction(text, positive_counts, prob_positive, positive_review_count)\n",
    "if pos>neg:\n",
    "   print (\"%s - Positive (pos score={0}, neg score={1})\".format(pos,neg) %(text))\n",
    "else:\n",
    "   print (\"%s - Negative (pos score={0}, neg score={1})\".format(pos,neg) %(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Faster way to predict using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jasabanta: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "! sudo pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial naive bayes AUC: 0.7850859656137545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "test=[]\n",
    "actual=[]\n",
    "for fileName in os.listdir(\"mr/test/pos\"):\n",
    "    fo=open(\"mr/test/pos/%s\" % fileName,\"r\")\n",
    "    str= fo.read()\n",
    "    test.append(str)\n",
    "    actual.append(1)\n",
    "for fileName in os.listdir(\"mr/test/neg\"):\n",
    "    fo=open(\"mr/test/neg/%s\" % fileName,\"r\")\n",
    "    str= fo.read()\n",
    "    test.append(str)\n",
    "    actual.append(-1)\n",
    "reviews = []\n",
    "for r in posreviews:\n",
    "    reviews.append(r)\n",
    "for r in negreviews:\n",
    "    reviews.append(r)\n",
    "# Generate counts from text using a vectorizer.  There are other vectorizers available, and lots of options you can set.\n",
    "# This performs our step of computing word counts.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_features = vectorizer.fit_transform([r for r in reviews])\n",
    "test_features = vectorizer.transform([r for r in test])\n",
    "\n",
    "# Fit a naive bayes model to the training data.\n",
    "# This will train the model using the word counts we computer, and the existing classifications in the training set.\n",
    "nb = MultinomialNB()\n",
    "trainRes=[]\n",
    "for r in posreviews:\n",
    "    trainRes.append(1)\n",
    "for r in negreviews:\n",
    "    trainRes.append(-1)\n",
    "nb.fit(train_features,trainRes)\n",
    "\n",
    "# Now we can use the model to predict classifications for our test features.\n",
    "predictions = nb.predict(test_features)\n",
    "#print predictions\n",
    "# Compute the error.  It is slightly different from our model because the internals of this process work differently from our implementation.\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "\n",
    "#print('FPR', ' - ', fpr)\n",
    "#print ('TPR', ' - ', tpr)\n",
    "#print (thresholds)\n",
    "print(\"Multinomial naive bayes AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
