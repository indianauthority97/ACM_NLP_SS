{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Introduction to WordNet and Word semantics</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Distributional Semantics</h2>\n",
    "\n",
    "Distributional semantics is a research area that develops and studies theories and methods for quantifying and categorizing semantic similarities between linguistic items based on their distributional properties in large samples of language data.<br>\n",
    "The basic idea of distributional semantics can be summed up in the so-called Distributional hypothesis: linguistic items with similar distributions have similar meanings.[1]\n",
    "\n",
    "<h4>Distributional hypothesis</h4>\n",
    "\n",
    "The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings.<br>\n",
    "The underlying idea that \"a word is characterized by the company it keeps\" was popularized by Firth.<br>\n",
    "Although the Distributional Hypothesis originated in linguistics, it is now receiving attention in cognitive science especially regarding the context of word use.<br>\n",
    "The distributional hypothesis suggests that the more semantically similar two words are, the more distributionally similar they will be in turn, and thus the more that they will tend to occur in similar linguistic contexts. <br>\n",
    "\n",
    "<h4>Distributional semantic modeling</h4>\n",
    "\n",
    "Distributional semantics favor the use of linear algebra as computational tool and representational framework. The basic approach is to collect distributional information in high-dimensional vectors, and to define distributional/semantic similarity in terms of vector similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Similarity measures for binary vectors</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X and Y denote the binary distributional vectors for words X and Y. [2]\n",
    "\n",
    "$$Dice Cofficient: \\frac{2 \\mid \\vec{X} \\cap \\vec{Y} \\mid}{\\mid \\vec{X} \\mid + \\mid \\vec{Y} \\mid}$$\n",
    "$$Jaccard Cofficient: \\frac{\\mid \\vec{X} \\cap \\vec{Y} \\mid}{\\mid \\vec{X} \\mid + \\mid \\vec{Y} \\mid}$$\n",
    "$$Overlap Cofficient: \\frac{2 \\mid \\vec{X} \\cup \\vec{Y} \\mid}{min( \\mid \\vec{X} \\mid , \\mid \\vec{Y} \\mid )}$$\n",
    "\n",
    "$$Cosine Similarity: Cos( \\vec{X} , \\vec{Y} ) = \\frac{\\vec{X} \\cdot \\vec{Y} }{ \\mid \\vec{X} \\mid \\mid \\vec{Y} \\mid} \n",
    "$$\n",
    "\n",
    "<b>How to get distributional vectors (X and Y)? - Word2Vec </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word2Vec</h3>\n",
    "- Word2vec is a group of related models that are used to produce word embeddings. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.[3]\n",
    "- Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "- Word2vec can utilize either of two model architectures to produce a distributed representation of words:\n",
    "                - Continuous bag-of-words (CBOW) \n",
    "                - Continuous skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. CBOW</h4>\n",
    "\n",
    "- In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Skip Gram</h4>\n",
    "- skip-grams are a generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are skipped over. They provide one way of overcoming the data sparsity problem found with conventional n-gram analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Relational Similarity</h3>\n",
    "\n",
    "- Two pairs (a,b) and (c,d) are said to be relationally similar if they have many similar relations. <br>\n",
    "                Example- king:man and queen:woman\n",
    "- Realtional semilarities can be exploited using vector arithmetic which is show in following example. Here we are performing vector addition of vectors of 'king' and 'woman' and subtacting 'queen' vector and the result we are getting is most similar to 'men'\n",
    "- Read more on relational semantics and word analogy on\n",
    "    https://levyomer.wordpress.com/2014/04/25/linguistic-regularities-in-sparse-and-explicit-word-representations/ [4]\n",
    "- Dataset description: Dataset taken is pre trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in [2]. We have extracted about 50-70 vectors out of original model because it is easy for explanation purpose. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: \n",
      "0.758035028102\n"
     ]
    }
   ],
   "source": [
    "#Example showing vector arithmetic to exploit word similarities\n",
    "#Precomputed vectors data taken from: https://code.google.com/archive/p/word2vec/\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "dictionary = dict()\n",
    "with open(\"Data/IntroductiontoWordNetandWordsemantics/extracted_vectors.txt\",\"r\") as f: #sentences is the data set we have used\n",
    "    for line in f:\n",
    "        temp = line.strip(\"\\n\").split(\",\")\n",
    "        vector = temp[1].split(\";\")\n",
    "        vector = [float(i) for i in vector]\n",
    "        dictionary[temp[0]] = vector\n",
    "king = np.asarray(dictionary['king'])\n",
    "queen = np.asarray(dictionary['queen'])\n",
    "man = np.asarray(dictionary['man'])\n",
    "woman = np.asarray(dictionary['woman'])\n",
    "water = np.asarray(dictionary['water'])\n",
    "\n",
    "#simiarity between (vector(king)-vector(man)) and (vector(queen)-vector(woman))\n",
    "similarity = 1 - spatial.distance.cosine(king-man,queen-woman)\n",
    "print ('Similarity: ')\n",
    "print (similarity)\n",
    "\n",
    "#similarly you can provide---\n",
    "#hot + weak - cold = strong\n",
    "#France + Athens - Paris = Greece/Greek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following code snippet, we are showing cosine distance of 'king' with 'queen','man', and 'woman'.\n",
    "- The resultant graph shows that since the cosine distance of 'king' and 'queen' is less,thus 'king' is more closely related to 'queen' than 'man' and at last 'woman', taking vectors of these words mentioned above in notebook.\n",
    "- For this install 'matplotlib':\n",
    "                pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. WordNet :</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser.WordNet's structure makes it a useful tool for computational linguistics and natural language processing.[5]\n",
    "\n",
    "WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However, there are some important distinctions. First, WordNet interlinks not just word forms—strings of letters—but specific senses of words. As a result, words that are found in close proximity to one another in the network are semantically disambiguated. Second, WordNet labels the semantic relations among words, whereas the groupings of words in a thesaurus does not follow any explicit pattern other than meaning similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Application of wordnet :</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet has been used for a number of different purposes in information systems, including word-sense disambiguation, information retrieval, automatic text classification, automatic text summarization, machine translation and even automatic crossword puzzle generation.\n",
    "\n",
    "A common use of WordNet is to determine the similarity between words. Various algorithms have been proposed, and these include measuring the distance among the words and synsets in WordNet's graph structure, such as by counting the number of edges among synsets. The intuition is that the closer two words or synsets are, the closer their meaning. A number of WordNet-based word similarity algorithms are implemented in a Perl package called WordNet::Similarity and in a Python package called NLTK. Other more sophisticated WordNet-based similarity techniques include ADW, whose implementation is available in Java. WordNet can also be used to inter-link other vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Downloading WordNet</h5>\n",
    "- You can download wordNet corpus from nltk. Run form python terminal\n",
    "                            - import nltk\n",
    "                            - nltk.download()\n",
    "  and from the dialog-box, download 'wordnet' corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h5>Synonyms of a word</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rs/jpatro/anaconda3/lib/python3.6/tkinter/__init__.py\", line 1699, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/home/rs/jpatro/anaconda3/lib/python3.6/tkinter/__init__.py\", line 745, in callit\n",
      "    func(*args)\n",
      "  File \"/home/rs/jpatro/anaconda3/lib/python3.6/site-packages/nltk/downloader.py\", line 1914, in _monitor_message_queue\n",
      "    self._select(msg.package.id)\n",
      "AttributeError: 'str' object has no attribute 'id'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Example showing synonyms\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "syns = wordnet.synsets(\"good\")\n",
    "\n",
    "for syn in syns:\n",
    "    print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just the first synonym :  Synset('good.n.01')\n",
      "Only the name :  good\n",
      "\n",
      "\n",
      "Definition:\n",
      "benefit\n",
      "\n",
      "\n",
      "Examples:\n",
      "[u'for your own good', u\"what's the good of worrying?\"]\n"
     ]
    }
   ],
   "source": [
    "#In order to print just first synonym\n",
    "print \"Just the first synonym : \" , syns[0]\n",
    "print \"Only the name : \" , syns[0].lemmas()[0].name()\n",
    "print \"\\n\"\n",
    "\n",
    "#In order to print the definition:\n",
    "print (\"Definition:\")\n",
    "print (syns[0].definition())\n",
    "print \"\\n\"\n",
    "\n",
    "#In order to print the examples:\n",
    "print \"Examples:\"\n",
    "print syns[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Similarity between boat and sheep', 0.9090909090909091)\n",
      "('Similarity between boat and cat', 0.32)\n",
      "('Similarity between boat and car', 0.6956521739130435)\n"
     ]
    }
   ],
   "source": [
    "w1=wordnet.synset(\"boat.n.01\")\n",
    "w2=wordnet.synset(\"ship.n.01\")\n",
    "print (\"Similarity between boat and sheep\", w1.wup_similarity(w2))\n",
    "\n",
    "w1=wordnet.synset(\"boat.n.01\")\n",
    "w2=wordnet.synset(\"cat.n.01\")\n",
    "print (\"Similarity between boat and cat\", w1.wup_similarity(w2))\n",
    "\n",
    "w1=wordnet.synset(\"boat.n.01\")\n",
    "w2=wordnet.synset(\"car.n.01\")\n",
    "print (\"Similarity between boat and car\", w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Finding similarity between any two words:</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References:</h3>\n",
    "<pre>\n",
    "[1]Distributional semantics https://en.wikipedia.org/wiki/Distributional_semantics\n",
    "[2]Pawan Goyal, Lecture slides, Speech and natural language processing (CS60057), Autumn 2016, IIT Kharagpur.\n",
    "[3]Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).\n",
    "[4]Levy, Omer, Yoav Goldberg, and Israel Ramat-Gan. \"Linguistic Regularities in Sparse and Explicit Word Representations.\" CoNLL. 2014.\n",
    "[5]About Wordnet, https://wordnet.princeton.edu/\n",
    "[6]Word-sense disambiguation\n",
    "[7]Liling Tan. 2014. Pywsd: Python Implementations of Word Sense Disambiguation (WSD) Technologies [software]. Retrieved from https://github.com/alvations/pywsd\n",
    "[8]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
